{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_inclass_w19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Rh4uU7eob-bc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<center>\n",
        "Final Exam\n",
        "</center>\n",
        "</h1>\n",
        "<div class=h1_cell>\n",
        "<p>\n",
        "The final has 3 basic parts. Part 1 is wrangling a dataset and using ann_flex to train and test a model (40 points). Part 2 is to look at the results of testing in part 1 and do some filtering on the test samples (30 points). Part 3 is an exploration of an algorithm that tries to reduce weight updates (30 points). It uses a simulation loop.\n",
        " <p>\n",
        "   I will give you the dataset to use. You have to supply your library."
      ]
    },
    {
      "metadata": {
        "id": "BQcquKEsbB0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm library_w19_deep_2.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wl7zyWU-bG55",
        "colab_type": "code",
        "outputId": "9c497783-23ff-4c31-bbc6-cd84f1696631",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ad55734-218c-4dc1-b3aa-328670dd6367\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1ad55734-218c-4dc1-b3aa-328670dd6367\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving library_w19_deep_2.py to library_w19_deep_2.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'library_w19_deep_2.py': b\"import numpy as np\\r\\n\\r\\ndef sigmoid(x):  \\r\\n    return 1/(1+np.exp(-x))\\r\\n\\r\\ndef mse(z,y):\\r\\n  return (z-y)**2\\r\\n\\r\\ndef mse_der(z,y):\\r\\n  return z-y\\r\\n\\r\\ndef sigmoid_der(x):  \\r\\n    return sigmoid(x)*(1-sigmoid(x))\\r\\n\\r\\ndef ann_simple(all_samples, labels, weights, bias, hypers={}):\\r\\n  \\r\\n  '''\\r\\n  Can build an ANN with n input nodes and one output node.\\r\\n  Uses sigmoid and mse.\\r\\n  '''\\r\\n  \\r\\n  input_n = all_samples.shape[1]  #number of inputs in each sample\\r\\n  \\r\\n  assert weights.shape == (input_n,1), 'weights needs to have same shape as sample'\\r\\n  assert all_samples.shape[0] >= 1, 'all_samples must represent 1 or more samples'\\r\\n  assert bias.shape == (1,) , 'a single bias weight for output node'\\r\\n  assert labels.shape[1] == 1, 'actual value for the 1 output node'\\r\\n  assert labels.shape[0] == all_samples.shape[0], 'labels must match up with samples'\\r\\n  \\r\\n  hyper_keys = [*hypers]  #fails on 2.7\\r\\n  target_set = set(['epochs', 'cost-reporting', 'learning-rate'])  #might add more later\\r\\n  diff_set = set(hyper_keys) - target_set\\r\\n  if diff_set: print('WARNING: unrecognized hyper parameters ' + str(diff_set))\\r\\n\\r\\n  max_epochs = hypers['epochs'] if 'epochs' in hypers else 100\\r\\n  cost_reporting = hypers['cost-reporting'] if 'cost-reporting' in hypers else 100  #how often to report epoch cost\\r\\n  alpha = hypers['learning-rate'] if 'learning-rate' in hypers else .05\\r\\n  \\r\\n  cost_accumulator = [0, 0]  #[count, sum] use to print out costs now and then\\r\\n  \\r\\n  for i in range(max_epochs):\\r\\n\\r\\n    #Go through each sample forward and backward\\r\\n    for j in range(len(all_samples)):\\r\\n\\r\\n\\r\\n      #do forward propogation\\r\\n      sample = np.expand_dims(all_samples[j], axis=1) #transform to match up with weight shape\\r\\n      XW = np.multiply(sample, weights)\\r\\n      XW_sum = np.sum(XW, axis=0)\\r\\n      raw_output = XW_sum + bias\\r\\n      z = sigmoid(raw_output)  #what we are predicting\\r\\n\\r\\n      #compute error\\r\\n      cost = mse(z, labels[j])\\r\\n      cost_accumulator[0] += 1  #use to print out\\r\\n      cost_accumulator[1] += cost  #use to print out\\r\\n\\r\\n      #back propogation\\r\\n      mse_deriv_value = mse_der(z, labels[j])\\r\\n      sigmoid_deriv_value = sigmoid_der(raw_output)\\r\\n      z_delta = mse_deriv_value * sigmoid_deriv_value\\r\\n\\r\\n      #update weights - notice z_delta part of each update\\r\\n      for k in range(len(weights)):\\r\\n        weights[k][0] -= alpha * all_samples[j][k] * z_delta\\r\\n        \\r\\n      #update bias\\r\\n      bias -=  1.0*z_delta\\r\\n\\r\\n    #print ith cost value\\r\\n    if i%cost_reporting == 0:\\r\\n      average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\r\\n      print((i,average_cost))\\r\\n      cost_accumulator = [0, 0]  #reset\\r\\n  #end epoch loop\\r\\n  \\r\\n  if cost_accumulator[0]:\\r\\n    average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\r\\n    print((max_epochs,average_cost))\\r\\n    \\r\\n  return (weights,bias)  #don't lose these! They are the whole model.\\r\\n\\r\\ndef from_scratch(samples, labels, hypers):\\r\\n  \\r\\n  input_n = samples.shape[1]\\r\\n  \\r\\n  #reset weights to initial values. Seed of 42 guarantees same random values\\r\\n  np.random.seed(42)\\r\\n  weights = np.random.rand(input_n,1)  #elasticity in action\\r\\n  bias = np.random.rand(1)\\r\\n  \\r\\n  return ann_simple(samples, labels, weights, bias, hypers)\\r\\n\\r\\ndef ann_predictor(sample, weights, bias):\\r\\n  \\r\\n  s2 = np.expand_dims(sample, axis=1)\\r\\n  XW = np.multiply(s2, weights)\\r\\n  XW_sum = np.sum(XW, axis=0)\\r\\n  raw_output = XW_sum + bias\\r\\n  z = sigmoid(raw_output)\\r\\n\\r\\n  return 1 if z > .5 else 0  #.5 should probably be a parameter\\r\\n\\r\\ndef ann_tester(samples, labels, weights, bias):\\r\\n  weights = np.array(weights)\\r\\n  bias = np.array(bias)\\r\\n  \\r\\n  predictions = [ann_predictor(s, weights, bias) for s in samples]\\r\\n  zipped = list(zip(predictions, labels))\\r\\n  \\r\\n  return (zipped.count((1,1)) + zipped.count((0,0)))/len(zipped)\\r\\n\\r\\ndef from_scratch_batch(samples, labels, hypers):\\r\\n  \\r\\n  input_n = samples.shape[1]\\r\\n  \\r\\n  #reset weights to initial values. Seed of 42 guarantees same random values\\r\\n  np.random.seed(42)\\r\\n  weights = np.random.rand(input_n,1)\\r\\n  bias = np.random.rand(1)\\r\\n  \\r\\n  return ann_simple_batch(samples, labels, weights, bias, hypers)\\r\\n\\r\\ndef ann_simple_batch(all_samples, labels, weights, bias, hypers={}):\\r\\n  '''\\r\\n  Can build an ANN with n input nodes and one output node.\\r\\n  Uses sigmoid and mse.\\r\\n  '''\\r\\n\\r\\n\\r\\n  input_n = all_samples.shape[1]  #number of inputs in each sample\\r\\n\\r\\n\\r\\n  assert weights.shape == (input_n,1), 'weights needs to have same shape as sample'\\r\\n  assert all_samples.shape[0] >= 1, 'all_samples must represent 1 or more samples'\\r\\n  assert bias.shape == (1,) , 'a single bias weight for output node'\\r\\n  assert labels.shape[1] == 1, 'actual value for the 1 output node'\\r\\n  assert labels.shape[0] == all_samples.shape[0], 'labels must match up with samples'\\r\\n\\r\\n\\r\\n  hyper_keys = [*hypers]  #fails on 2.7\\r\\n  target_set = set(['epochs', 'cost-reporting', 'learning-rate', 'batch-size'])  #might add more later\\r\\n  diff_set = set(hyper_keys) - target_set\\r\\n  if diff_set: print('WARNING: unrecognized hyper parameters ' + str(diff_set))\\r\\n\\r\\n\\r\\n  max_epochs = hypers['epochs'] if 'epochs' in hypers else 100\\r\\n  cost_reporting = hypers['cost-reporting'] if 'cost-reporting' in hypers else 100  #how often to report epoch cost\\r\\n  alpha = hypers['learning-rate'] if 'learning-rate' in hypers else .05  #learning rate\\r\\n  batch_n = min(hypers['batch-size'],len(all_samples))  if 'batch-size' in hypers else 1  #batch size defaults to 1\\r\\n  tW = [0] * len(weights)\\r\\n  tBias = 0\\r\\n  cost_accumulator = [0, 0]\\r\\n  batch_count = 0\\r\\n  for i in range(max_epochs):\\r\\n    batch_count += 1\\r\\n    \\r\\n    for j in range(len(all_samples)):\\r\\n      sample = np.expand_dims(all_samples[j], axis=1)\\r\\n      XW = np.multiply(sample, weights)\\r\\n      XW_sum = np.sum(XW, axis=0)\\r\\n      raw_output = XW_sum + bias\\r\\n      z = sigmoid(raw_output)\\r\\n      \\r\\n      cost = mse(z, labels[j])\\r\\n      cost_accumulator[0] += 1\\r\\n      cost_accumulator[1] += cost\\r\\n      \\r\\n      mse_deriv_value = mse_der(z, labels[j])\\r\\n      sigmoid_deriv_value = sigmoid_der(raw_output)\\r\\n      z_delta = mse_deriv_value * sigmoid_deriv_value\\r\\n      \\r\\n      for k in range(len(tW)):\\r\\n        tW += alpha * all_samples[j][k] * z_delta\\r\\n        \\r\\n      tBias += 1.0 * z_delta\\r\\n    if (batch_count == batch_n):\\r\\n      for i in range(len(tW)):\\r\\n        weights[i][0] = tW[i] / batch_n\\r\\n        bias = tBias\\r\\n      batch_count = 1\\r\\n    tBias = 0\\r\\n    tW = [0] * len(weights)\\r\\n    \\r\\n    if i%cost_reporting == 0:\\r\\n      average_cost = cost_accumulator[1]/cost_accumulator[0]\\r\\n      print((i,average_cost))\\r\\n      cost_accumulator = [0, 0]\\r\\n    \\r\\n  if cost_accumulator[0]:\\r\\n    average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\r\\n    print((max_epochs,average_cost))\\r\\n    \\r\\n  return (weights, bias)\\r\\n\\r\\ndef ann_flex(all_samples, all_labels, weights, biases, hypers={}):\\r\\n  \\r\\n  '''\\r\\n  Can build an ANN with n input nodes and m output nodes.\\r\\n  Uses sigmoid and mse.\\r\\n  '''\\r\\n  \\r\\n  input_n = all_samples.shape[1]  #number of inputs in each sample\\r\\n  output_n = all_labels.shape[1]      #number of outputs in each label\\r\\n  \\r\\n  assert weights.shape == (input_n,output_n), 'weights needs to have same shape as sample'\\r\\n  assert all_samples.shape[0] >= 1, 'all_samples must represent 1 or more samples'\\r\\n  assert biases.shape == (output_n,) , 'a bias weight for each output node'\\r\\n  assert all_labels.shape[0] == all_samples.shape[0], 'labels must match up with samples'\\r\\n  \\r\\n  hyper_keys = [*hypers]  #fails on 2.7\\r\\n  target_set = set(['epochs', 'cost-reporting', 'learning-rate'])  #might add more later\\r\\n  diff_set = set(hyper_keys) - target_set\\r\\n  if diff_set: print('WARNING: unrecognized hyper parameters ' + str(diff_set))\\r\\n\\r\\n  max_epochs = hypers['epochs'] if 'epochs' in hypers else 100\\r\\n  cost_reporting = hypers['cost-reporting'] if 'cost-reporting' in hypers else 100  #how often to report epoch cost\\r\\n  alpha = hypers['learning-rate'] if 'learning-rate' in hypers else .05\\r\\n  cost_accumulator = [0, 0]\\r\\n  \\r\\n  for i in range(max_epochs):\\r\\n\\r\\n    #Go through each sample forward and backward\\r\\n    for j in range(len(all_samples)):\\r\\n\\r\\n\\r\\n      #do forward propogation\\r\\n      sample = np.array(all_samples[j]) #transform to match up with weight shape\\r\\n      raw = sample.dot(weights)\\r\\n      full_raw = np.add(raw, biases)\\r\\n      fsig = np.vectorize(sigmoid)\\r\\n      zv = fsig(full_raw)  #what we are predicting\\r\\n\\r\\n      #compute error\\r\\n      label = np.array(all_labels[j])\\r\\n      costs = np.array([mse(a, b) for (a, b) in zip(zv, label)])\\r\\n      cost_accumulator[0] += 1  #use to print out\\r\\n      cost_accumulator[1] += costs  #use to print out\\r\\n\\r\\n      #back propogation\\r\\n      mse_deriv_values = np.array([mse_der(a, b) for (a, b) in zip(zv, label)])\\r\\n      fsig_der = np.vectorize(sigmoid_der)\\r\\n      sigmoid_deriv_values = fsig_der(full_raw)\\r\\n      z_deltas = np.multiply(mse_deriv_values, sigmoid_deriv_values)\\r\\n\\r\\n      #update weights - notice z_delta part of each update\\r\\n      weight_changes = [sample * z_deltas[i] for i in range(len(z_deltas))]\\r\\n      wt = np.transpose(weight_changes)\\r\\n      weights = np.subtract(weights, wt)\\r\\n        \\r\\n      #update bias\\r\\n      biases = np.subtract(biases, z_deltas)\\r\\n\\r\\n    #print ith cost value\\r\\n    if i%cost_reporting == 0:\\r\\n      average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\r\\n      average_cost = np.array(average_cost)\\r\\n      print((i,sum(average_cost)))\\r\\n      cost_accumulator = [0, 0]  #reset\\r\\n  #end epoch loop\\r\\n  \\r\\n  if cost_accumulator[0]:\\r\\n    average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\r\\n    print((max_epochs, average_cost))\\r\\n    \\r\\n  return (weights,biases)\\r\\n \\r\\ndef from_scratch_flex(samples, labels, hypers):\\r\\n  \\r\\n  input_n = samples.shape[1]\\r\\n  output_n = labels.shape[1]\\r\\n  \\r\\n  #reset weights to initial values. Seed of 42 guarantees same random values\\r\\n  np.random.seed(42)\\r\\n  weights = .2*np.random.rand(input_n,output_n) - .1\\r\\n  biases = np.random.rand(output_n)\\r\\n  \\r\\n  return ann_flex(samples, labels, weights, biases, hypers)\\r\\n\\r\\ndef ann_flex_predictor(sample, weights, biases):\\r\\n  fsig = np.vectorize(sigmoid)\\r\\n  \\r\\n  #do forward propogation\\r\\n  raw = sample.dot(weights)\\r\\n  full_raw = np.add(raw, biases)\\r\\n  final_output = fsig(full_raw) #an array of values for output nodes\\r\\n\\r\\n  return final_output\\r\\n\\r\\ndef ann_flex_tester(samples,  weights, biases):\\r\\n  \\r\\n  predictions = [ann_flex_predictor(s, weights, biases) for s in samples] #an array of arrays\\r\\n  \\r\\n  return predictions\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "G8rLH27bbevq",
        "colab_type": "code",
        "outputId": "a21fb2f3-9aaf-4007-cc5f-42bcb8541a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "from library_w19_deep_2 import *\n",
        "\n",
        "%who function"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ann_flex\t ann_flex_predictor\t ann_flex_tester\t ann_predictor\t ann_simple\t ann_simple_batch\t ann_tester\t from_scratch\t from_scratch_batch\t \n",
            "from_scratch_flex\t mse\t mse_der\t sigmoid\t sigmoid_der\t \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zn2536_DVRFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8IP18ATeIua5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTST3CURFkC0lHSfR9mGzBAvCmLWl06kw9NSBC3UuD9hA1SlPjh807BRxfnvhq5hrFyRz5ZCTGrs7fr/pub?output=csv'\n",
        "movie_table = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hl0A9_GyQGG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ff832e98-85c5-47b9-d22f-a81697fd4f26"
      },
      "cell_type": "code",
      "source": [
        "movie_table = movie_table[:200]  #trim it down to 200 rows\n",
        "len(movie_table)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "Lk5iS4oaQXjR",
        "colab_type": "code",
        "outputId": "48e1c003-4373-42db-ca7f-6baa68187ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "movie_table.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>color</th>\n",
              "      <th>director_name</th>\n",
              "      <th>num_critic_for_reviews</th>\n",
              "      <th>duration</th>\n",
              "      <th>director_facebook_likes</th>\n",
              "      <th>actor_3_facebook_likes</th>\n",
              "      <th>actor_2_name</th>\n",
              "      <th>actor_1_facebook_likes</th>\n",
              "      <th>gross</th>\n",
              "      <th>genres</th>\n",
              "      <th>...</th>\n",
              "      <th>num_user_for_reviews</th>\n",
              "      <th>language</th>\n",
              "      <th>country</th>\n",
              "      <th>content_rating</th>\n",
              "      <th>budget</th>\n",
              "      <th>title_year</th>\n",
              "      <th>actor_2_facebook_likes</th>\n",
              "      <th>imdb_score</th>\n",
              "      <th>aspect_ratio</th>\n",
              "      <th>movie_facebook_likes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Color</td>\n",
              "      <td>James Cameron</td>\n",
              "      <td>723.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>855.0</td>\n",
              "      <td>Joel David Moore</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>760505847.0</td>\n",
              "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
              "      <td>...</td>\n",
              "      <td>3054.0</td>\n",
              "      <td>English</td>\n",
              "      <td>USA</td>\n",
              "      <td>PG-13</td>\n",
              "      <td>237000000.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>936.0</td>\n",
              "      <td>7.9</td>\n",
              "      <td>1.78</td>\n",
              "      <td>33000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Color</td>\n",
              "      <td>Gore Verbinski</td>\n",
              "      <td>302.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>563.0</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>Orlando Bloom</td>\n",
              "      <td>40000.0</td>\n",
              "      <td>309404152.0</td>\n",
              "      <td>Action|Adventure|Fantasy</td>\n",
              "      <td>...</td>\n",
              "      <td>1238.0</td>\n",
              "      <td>English</td>\n",
              "      <td>USA</td>\n",
              "      <td>PG-13</td>\n",
              "      <td>300000000.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>7.1</td>\n",
              "      <td>2.35</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Color</td>\n",
              "      <td>Sam Mendes</td>\n",
              "      <td>602.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>Rory Kinnear</td>\n",
              "      <td>11000.0</td>\n",
              "      <td>200074175.0</td>\n",
              "      <td>Action|Adventure|Thriller</td>\n",
              "      <td>...</td>\n",
              "      <td>994.0</td>\n",
              "      <td>English</td>\n",
              "      <td>UK</td>\n",
              "      <td>PG-13</td>\n",
              "      <td>245000000.0</td>\n",
              "      <td>2015.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>6.8</td>\n",
              "      <td>2.35</td>\n",
              "      <td>85000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Color</td>\n",
              "      <td>Christopher Nolan</td>\n",
              "      <td>813.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>22000.0</td>\n",
              "      <td>23000.0</td>\n",
              "      <td>Christian Bale</td>\n",
              "      <td>27000.0</td>\n",
              "      <td>448130642.0</td>\n",
              "      <td>Action|Thriller</td>\n",
              "      <td>...</td>\n",
              "      <td>2701.0</td>\n",
              "      <td>English</td>\n",
              "      <td>USA</td>\n",
              "      <td>PG-13</td>\n",
              "      <td>250000000.0</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>23000.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>2.35</td>\n",
              "      <td>164000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>Doug Walker</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Rob Walker</td>\n",
              "      <td>131.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Documentary</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   color      director_name  num_critic_for_reviews  duration  \\\n",
              "0  Color      James Cameron                   723.0     178.0   \n",
              "1  Color     Gore Verbinski                   302.0     169.0   \n",
              "2  Color         Sam Mendes                   602.0     148.0   \n",
              "3  Color  Christopher Nolan                   813.0     164.0   \n",
              "4    NaN        Doug Walker                     NaN       NaN   \n",
              "\n",
              "   director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
              "0                      0.0                   855.0  Joel David Moore   \n",
              "1                    563.0                  1000.0     Orlando Bloom   \n",
              "2                      0.0                   161.0      Rory Kinnear   \n",
              "3                  22000.0                 23000.0    Christian Bale   \n",
              "4                    131.0                     NaN        Rob Walker   \n",
              "\n",
              "   actor_1_facebook_likes        gross                           genres  \\\n",
              "0                  1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi   \n",
              "1                 40000.0  309404152.0         Action|Adventure|Fantasy   \n",
              "2                 11000.0  200074175.0        Action|Adventure|Thriller   \n",
              "3                 27000.0  448130642.0                  Action|Thriller   \n",
              "4                   131.0          NaN                      Documentary   \n",
              "\n",
              "          ...          num_user_for_reviews language  country  content_rating  \\\n",
              "0         ...                        3054.0  English      USA           PG-13   \n",
              "1         ...                        1238.0  English      USA           PG-13   \n",
              "2         ...                         994.0  English       UK           PG-13   \n",
              "3         ...                        2701.0  English      USA           PG-13   \n",
              "4         ...                           NaN      NaN      NaN             NaN   \n",
              "\n",
              "        budget  title_year actor_2_facebook_likes imdb_score  aspect_ratio  \\\n",
              "0  237000000.0      2009.0                  936.0        7.9          1.78   \n",
              "1  300000000.0      2007.0                 5000.0        7.1          2.35   \n",
              "2  245000000.0      2015.0                  393.0        6.8          2.35   \n",
              "3  250000000.0      2012.0                23000.0        8.5          2.35   \n",
              "4          NaN         NaN                   12.0        7.1           NaN   \n",
              "\n",
              "  movie_facebook_likes  \n",
              "0                33000  \n",
              "1                    0  \n",
              "2                85000  \n",
              "3               164000  \n",
              "4                    0  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "an3i3EhpkywZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Question 1: get your wrangling boots on (15 points)</h2>\n",
        "<p>\n",
        "I am going to tell you what I want and then let you figure out the steps to get there.\n",
        "  <p>\n",
        "   1. I wanti `mdb_score` to be binned into 3 values using `qcut`. I then want that one-hot encoded into the list of labels for `ann_flex`. FInal shape of your `labels` is (n,3).\n",
        "   2. I would like to use the following columns as part of a sample: `facenumber_in_poster,\tnum_user_for_reviews,\tbudget`. Please normalize these columns.\n",
        "   3. I would like to use  `language` as also part of a sample.\n",
        "   4. You should end up with 4 values in each sample.\n",
        " <br><p> \n",
        "Your `ann_flex` function will choke on `NaN` values so make sure you remove rows that contain them.\n",
        "   <p>\n",
        "When you have `feature_set` (the list of samples) and `labels` ready to go, I'll meet you at the next question.\n",
        "  \n"
      ]
    },
    {
      "metadata": {
        "id": "3XBwrDNgC-oM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "movie_table = movie_table.dropna()\n",
        "labels = ['Low', 'Med', 'High']\n",
        "movie_table['imdb_binned'] = pd.qcut(movie_table['imdb_score'], 3, labels=labels)\n",
        "one_hot_imdb = pd.get_dummies(movie_table['imdb_binned'],prefix='imdb',dummy_na=False)  #no empties so use False\n",
        "movie_table = movie_table.join(one_hot_imdb)\n",
        "movie_table['fip_normed'] = movie_table['facenumber_in_poster']/movie_table['facenumber_in_poster'].max().astype(np.float64)\n",
        "movie_table['nufr_normed'] = movie_table['num_user_for_reviews']/movie_table['num_user_for_reviews'].max().astype(np.float64)\n",
        "movie_table['bud_normed'] = movie_table['budget']/movie_table['budget'].max().astype(np.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovu9hhiSa7xZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPCqExlkUz0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "fcb88eab-45fe-4cd9-c36a-43b01bd683da"
      },
      "cell_type": "code",
      "source": [
        "raw_table = movie_table.copy()\n",
        "ann_table = movie_table[['fip_normed', 'nufr_normed', 'bud_normed', 'language']].copy()\n",
        "ann_table['language'] = ann_table.apply(lambda row: 1 if row['language'] == 'English' else 0, axis=1)  \n",
        "test_labels = movie_table[['imdb_Low', 'imdb_Med', 'imdb_High']].copy()\n",
        "ann_table.head(1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fip_normed</th>\n",
              "      <th>nufr_normed</th>\n",
              "      <th>bud_normed</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.654382</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fip_normed  nufr_normed  bud_normed  language\n",
              "0         0.0     0.654382        0.79         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "AGqoQr8PSHZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "780f9553-20e1-4da6-91e0-650755d193da"
      },
      "cell_type": "code",
      "source": [
        "feature_set = ann_table.values.tolist()\n",
        "feature_set = np.array(feature_set)\n",
        "print(feature_set.shape)\n",
        "feature_set[:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(193, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.65438183, 0.79      , 1.        ],\n",
              "       [0.        , 0.26526677, 1.        , 1.        ],\n",
              "       [0.125     , 0.21298479, 0.81666667, 1.        ],\n",
              "       [0.        , 0.57874438, 0.83333333, 1.        ],\n",
              "       [0.125     , 0.15813156, 0.879     , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "4ygG8ZREC_Sy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "My feature set has this shape and first 5 values:\n",
        "<pre>\n",
        "(194, 4)\n",
        "array([[0.        , 0.64881341, 0.78894472, 1.        ],\n",
        "       [0.        , 0.25342913, 1.        , 1.        ],\n",
        "       [0.125     , 0.20030481, 0.81574539, 1.        ],\n",
        "       [0.        , 0.57195733, 0.83249581, 1.        ],\n",
        "       [0.125     , 0.14456782, 0.87839196, 1.        ]])\n",
        "       </pre>"
      ]
    },
    {
      "metadata": {
        "id": "ZzWKRykGM5Vk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "cac13e09-b80f-4f6a-f6dd-5f7482b9643b"
      },
      "cell_type": "code",
      "source": [
        "labels = np.array(test_labels)\n",
        "print(labels.shape)\n",
        "labels[0:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(193, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [0, 1, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "h-AhSHLkDSNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "My labels has this shape and first 5 values:\n",
        "<pre>\n",
        "(194, 3)\n",
        "array([[0, 0, 1],\n",
        "       [0, 1, 0],\n",
        "       [0, 1, 0],\n",
        "       [0, 0, 1],\n",
        "       [0, 1, 0]], dtype=uint8)\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "RmpGOC1aLqz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJjQrErgWcWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Question 2: train a model (15 points)</h2>\n",
        "\n",
        "Here's where you earn your 15 takehome points. Show that your `ann_flex` works correctly.\n",
        "<p>\n",
        "Train your model with the first 150 rows. Use `epochs` at 3000 and `cost-reporting` at whatever you like. Everything else can be default values. Here are my resulting `weights` and `biases`.\n",
        "<pre>\n",
        "array([[ 0.49499155, -1.51876998,  1.2508087 ],\n",
        "       [-7.17156319, -0.38214105,  3.18578869],\n",
        "       [-2.76749659, -1.26703916,  3.34669796],\n",
        "       [ 0.62092268,  0.17293429, -2.23830144]])\n",
        "array([ 1.41175081,  0.4811565 , -2.15045844])\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "vO8wp8k8zSY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "b7d2087d-97fc-479f-f754-2c5c3d4a4b4a"
      },
      "cell_type": "code",
      "source": [
        "(weights, biases) = from_scratch_flex(feature_set[:150], labels[:150], hypers={'epochs':3000, 'cost-reporting': 1000})"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 0.711989156200089)\n",
            "(1000, 0.6819104060697414)\n",
            "(2000, 0.681835817985708)\n",
            "(3000, array([0.22624216, 0.23840047, 0.21719318]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qWeCYYnZde_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "0c7c941e-64ad-4e3c-ffaf-568551abfc81"
      },
      "cell_type": "code",
      "source": [
        "weights, biases"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.49618789, -1.52004977,  1.25204887],\n",
              "        [-7.30285707, -0.39072486,  3.23561355],\n",
              "        [-2.78507374, -1.2693359 ,  3.36553471],\n",
              "        [ 0.68756875,  0.17857116, -2.27384152]]),\n",
              " array([ 1.47839688,  0.48679337, -2.18599852]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "upFVvbbYXLZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Question 3: test your model (10 points)</h2>\n",
        "\n",
        "You used the first 150 samples to train your model. Test your model with remaining samples from 200 you started with.\n",
        "<p>\n",
        "  For computing accuracy, you will have 3 raw values as output. Use the max value as the position of the 1. For instance, if you had output\n",
        "  <p>\n",
        "    `       array([0.55081372, 0.36331553, 0.11846781])`\n",
        "    <p>\n",
        "      you would predict '(1,0,0)`. The max value is in the 0th position so would place the 1 there. Once you have done this conversion, you can compare your predictions against labels to get accuracy.\n",
        "<p>\n",
        "        My accuracy is `0.36`. Just barely above the roll of a 3-sided dice.\n",
        "  <p>\n",
        "    One last note. You will need the raw output of your model in the next question so good to keep track of it here so you have it there.\n"
      ]
    },
    {
      "metadata": {
        "id": "U7-WTDTi3KYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5b90ea78-8e30-4db7-d15c-76686183fae7"
      },
      "cell_type": "code",
      "source": [
        "(weights, biases) = from_scratch_flex(feature_set, labels, hypers={'epochs':3000, 'cost-reporting': 1000})"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 0.7128927069257365)\n",
            "(1000, 0.680402403773903)\n",
            "(2000, 0.6802879669670459)\n",
            "(3000, array([0.23361516, 0.22474954, 0.22192326]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bZSTP_vJidGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "03727bcd-8b44-418b-d9df-3c8d5d8bac24"
      },
      "cell_type": "code",
      "source": [
        "raw_table['imdb_dummy'] = raw_table['imdb_binned']\n",
        "raw_table['imdb_dummy'].replace('High', 2, inplace=True)\n",
        "raw_table['imdb_dummy'].replace('Med', 1, inplace=True)\n",
        "raw_table['imdb_dummy'].replace('Low', 0, inplace=True)\n",
        "raw_table.head(1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>color</th>\n",
              "      <th>director_name</th>\n",
              "      <th>num_critic_for_reviews</th>\n",
              "      <th>duration</th>\n",
              "      <th>director_facebook_likes</th>\n",
              "      <th>actor_3_facebook_likes</th>\n",
              "      <th>actor_2_name</th>\n",
              "      <th>actor_1_facebook_likes</th>\n",
              "      <th>gross</th>\n",
              "      <th>genres</th>\n",
              "      <th>...</th>\n",
              "      <th>aspect_ratio</th>\n",
              "      <th>movie_facebook_likes</th>\n",
              "      <th>imdb_binned</th>\n",
              "      <th>imdb_Low</th>\n",
              "      <th>imdb_Med</th>\n",
              "      <th>imdb_High</th>\n",
              "      <th>fip_normed</th>\n",
              "      <th>nufr_normed</th>\n",
              "      <th>bud_normed</th>\n",
              "      <th>imdb_dummy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Color</td>\n",
              "      <td>James Cameron</td>\n",
              "      <td>723.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>855.0</td>\n",
              "      <td>Joel David Moore</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>760505847.0</td>\n",
              "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
              "      <td>...</td>\n",
              "      <td>1.78</td>\n",
              "      <td>33000</td>\n",
              "      <td>High</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.654382</td>\n",
              "      <td>0.79</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   color  director_name  num_critic_for_reviews  duration  \\\n",
              "0  Color  James Cameron                   723.0     178.0   \n",
              "\n",
              "   director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
              "0                      0.0                   855.0  Joel David Moore   \n",
              "\n",
              "   actor_1_facebook_likes        gross                           genres  \\\n",
              "0                  1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi   \n",
              "\n",
              "     ...     aspect_ratio movie_facebook_likes  imdb_binned  imdb_Low  \\\n",
              "0    ...             1.78                33000         High         0   \n",
              "\n",
              "  imdb_Med  imdb_High fip_normed nufr_normed  bud_normed imdb_dummy  \n",
              "0        0          1        0.0    0.654382        0.79          2  \n",
              "\n",
              "[1 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "5GenMIEfi0pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "37d694d6-d5e3-466a-da35-960649d1211e"
      },
      "cell_type": "code",
      "source": [
        "test_table_simp = raw_table.filter(['imdb_dummy'])\n",
        "test_list_simp = test_table_simp['imdb_dummy'].values.tolist()\n",
        "print(test_list_simp[:10])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1, 1, 2, 1, 0, 2, 2, 2, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j7l--GH8exmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "da6b5746-83c1-4804-9ff1-644dc726ef43"
      },
      "cell_type": "code",
      "source": [
        "predictions = ann_flex_tester(feature_set, weights, biases)\n",
        "preds_values = [np.argmax(p) for p in predictions]\n",
        "zipped = list(zip(preds_values, test_list_simp))\n",
        "sum([1 if p==a else 0 for p,a in zipped])/len(zipped)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45077720207253885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "k8WmOrRhW85U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Question 4:  filtering the test set (30 points)</h2>\n",
        "  \n",
        "I'd like to reduce the number of samples in the feature set I use for testing. I know we are only using a small number of test samples to keep processing costs down in the exam, but normally the size could be in the millions. What I would like to do is find the samples that have similar errors when doing prediction. Here is what I propose to do: look at pairwise sets of samples. If the 2 samples in the pair have roughly the same overall error (i.e., within epsilon), I can remove one of them from the test set: it is redundant.\n",
        "  <p>\n",
        "  I'm going to ask you to help me with the first part of this problem. I'd like an ordered list, pairwise, of all the samples along with the absolute difference in their overall errors. That's a mouthful. Let me give you some more details. I am using results we got from question 3.\n",
        "  \n",
        "  1. As you know, `labels` is a list of  triples where each triple is one-hot encoded.  Your raw output is also a triple of  values. What do I mean by \"raw output\"? I mean the actual 3 values that you get from your model, e.g., `array([0.55081372, 0.36331553, 0.11846781])`. For each such raw output, calculate the absolute difference between the 2 triples, i.e., your raw output triple and the label triple. This will give you a new triple of errors. So if the label for my example raw output above was `(0,0,1)`, I would get this as the error triple: `(abs(0-0.55081372), abs(0-0.36331553), abs(1-0.11846781))`. \n",
        "  2. Sum the error triple. That sum is what I want to store with the sample, i.e., I will record the sample index (or equivalently the label index) and a single summed error that goes with it. Note that this is different than the practice problem where you were doing an average. We are not doing any averaging.\n",
        "  3. I'd now like you to go pairwise through the samples and for each unique pair, record the absolute difference between their summed errors. As an example, if you were using a dictionary to keep track of your results, you might see an entry `(0,1): .003`, which signifies that the absolute difference between sample 0 and sample 1 is .003. What does that actually mean? The two samples are fairly close in their errors. Both errors could be quite high. But the absolute difference between them is small.\n",
        "  4. Order the list based on ascending absolute differences.\n",
        "\n",
        "  You can stop there. Normally I would then use your ordered list to filter out samples that have errors within some epsilon of each other. We do not need 2 separate samples that mimic each other in terms of errors they produce. We can just keep one of them and toss the other.\n",
        "<p>\n",
        "  You can see my final results below for the first 10 in my ordered list. To keep processing times down, I trimmed predictions and labels to [:15], i.e. the first 15.\n",
        "  <pre>\n",
        "  [((0, 3), 0.002331643103141934),\n",
        " ((7, 10), 0.01036188464574006),\n",
        " ((11, 12), 0.016973839107757627),\n",
        " ((0, 2), 0.028251848582276695),\n",
        " ((2, 3), 0.03058349168541863),\n",
        " ((5, 14), 0.0453548688750367),\n",
        " ((10, 14), 0.04570192275791962),\n",
        " ((6, 8), 0.05405101534935941),\n",
        " ((7, 14), 0.05606380740365968),\n",
        " ((5, 6), 0.06109838820689739)]\n",
        " </pre>\n",
        " <p>\n",
        "  What this means is that the total error for sample 0 and the total error for sample 3 are within 0.00233 of each other using absolute difference."
      ]
    },
    {
      "metadata": {
        "id": "GLsb3C7QBmGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To keep processing time within reason, only work with the first 15 predictions from your testing. Here are mine. If your first 15 don't match my first 15, go ahead and use mine."
      ]
    },
    {
      "metadata": {
        "id": "1Wh8d4YtkEa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_errors(raw, labels):\n",
        "  errors = []\n",
        "  errors2 = []\n",
        "  for i in range(len(labels)):\n",
        "    errors.append((abs(labels[i][0] - raw[i][0]), abs(labels[i][1] - raw[i][1]), abs(labels[i][2] - raw[i][2])))\n",
        "  for item in range(len(errors)):\n",
        "    errors2.append(np.sum(errors[item]))\n",
        "  return errors2\n",
        "    \n",
        "errors = calc_errors(full_raw, labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QwGWt2oGn4wb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "0ffd2a55-b11c-4d52-9b06-488fa69ba15d"
      },
      "cell_type": "code",
      "source": [
        "predictions_15 = [([0.55081372, 0.36331553, 0.11846781]),\n",
        " ([0.63322896, 0.5091635 , 0.06235565]),\n",
        " ([0.7097191 , 0.50770786, 0.04740651]),\n",
        " ([0.37215582, 0.45147866, 0.11034364]),\n",
        " ([0.45966018, 0.50442377, 0.08079772]),\n",
        " ([0.15938415, 0.47507109, 0.15527721]),\n",
        " ([0.09379516, 0.47731639, 0.18199096]),\n",
        " ([0.64372884, 0.28360809, 0.14045216]),\n",
        " ([0.66493551, 0.32965622, 0.1117983 ]),\n",
        " ([0.57558268, 0.5153202 , 0.06451307]),\n",
        " ([0.28899897, 0.45579208, 0.12019616]),\n",
        " ([0.63175644, 0.41794559, 0.08506059]),\n",
        " ([0.20354411, 0.49121776, 0.12801538]),\n",
        " ([0.70649204, 0.25956437, 0.13452677]),\n",
        " ([0.60337277, 0.37912699, 0.09892885])]\n",
        "predictions_15"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.55081372, 0.36331553, 0.11846781],\n",
              " [0.63322896, 0.5091635, 0.06235565],\n",
              " [0.7097191, 0.50770786, 0.04740651],\n",
              " [0.37215582, 0.45147866, 0.11034364],\n",
              " [0.45966018, 0.50442377, 0.08079772],\n",
              " [0.15938415, 0.47507109, 0.15527721],\n",
              " [0.09379516, 0.47731639, 0.18199096],\n",
              " [0.64372884, 0.28360809, 0.14045216],\n",
              " [0.66493551, 0.32965622, 0.1117983],\n",
              " [0.57558268, 0.5153202, 0.06451307],\n",
              " [0.28899897, 0.45579208, 0.12019616],\n",
              " [0.63175644, 0.41794559, 0.08506059],\n",
              " [0.20354411, 0.49121776, 0.12801538],\n",
              " [0.70649204, 0.25956437, 0.13452677],\n",
              " [0.60337277, 0.37912699, 0.09892885]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "TKwIzckMB7qk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Similarily, we need the first 15 labels from the test set. Here are mine. If your first 15 don't match my first 15, go ahead and use mine."
      ]
    },
    {
      "metadata": {
        "id": "xT5BYJb7oYRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "f251e285-577d-4a3d-bf75-0f7d23a5bfbb"
      },
      "cell_type": "code",
      "source": [
        "labels_15 = labels[:14]\n",
        "labels_15"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [0, 1, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 1, 0],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "YtdKa8cBdUgU",
        "colab_type": "code",
        "outputId": "8ba908da-1f40-4ac8-c785-6abed1dc2d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "labels_15"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [0, 0, 1],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0],\n",
              "       [1, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "metadata": {
        "id": "aKvudTPG5F7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "4eb93ab4-542e-468c-b536-5818bc7d37a6"
      },
      "cell_type": "code",
      "source": [
        "vals = []\n",
        "for i in range(len(labels_15)):\n",
        "  vals.append(calc_errors(labels_15[i], predictions_15[i]))\n",
        "  \n",
        "vals"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-8c02d6583994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_15\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_15\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-c950a5166872>\u001b[0m in \u001b[0;36mcalc_errors\u001b[0;34m(raw, labels)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0merrors2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0merrors2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Kij1-pe-gXOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Question 5: ignoring weight changes (30 points)</h2>\n",
        "\n",
        "For efficiency reasons, we may want to avoid making weight changes unless the change really matters. I'd like you to use a simulation loop to test out this algorithm:\n",
        "\n",
        " 1. Keep track of the weight changes from a previous sample. Unlike momentum, you need only to keep track of a single change matrix. I call this my history cache. Initially it will start with a zeroed out change matrix.\n",
        " 2. Compare the current weight change (simulated) with the weight change matrix in the history cache.\n",
        " 3. If they are close, do nothing; go immediately to the next sample. You can just ignore the weight change for the current sample and do nothing to the history cache.\n",
        " 4. If they are not close, then go ahead and make the (simulated) weight change to weights and store the current (simulated) weight change in your history cache, replacing what ever was there.\n",
        "<p>\n",
        "I hope you are asking what the heck does \"close\" mean. Well, I'll tell you. Those clever folks at numpy headquarters have defined a method to compare the closeness of 2 matrices. Here is what I would like you to use:\n",
        "<pre>\n",
        "  np.allclose(history_cache, simulated_changes, atol=.09)\n",
        "</pre>\n",
        "Pretty cool, huh. Normally `atol`, the tolerance the function uses, would be a hyper-parameter. I am just going to use .09.\n",
        "<p>\n",
        "I printed out some results for you to compare against. I printed out the starting weights and the ending weights. And every time I ignored a weight change I printed the sample index and the string `close`. Here is what I got. You can see I avoided 4 updates to weights.\n",
        "<pre>\n",
        "[[-0.02509198  0.09014286]\n",
        " [ 0.04639879  0.0197317 ]\n",
        " [-0.06879627 -0.0688011 ]]\n",
        "==============================\n",
        "(1, 'close')\n",
        "(2, 'close')\n",
        "(5, 'close')\n",
        "(9, 'close')\n",
        "array([[ 0.11476858, -0.0774422 ],\n",
        "       [ 0.20985929, -0.14123441],\n",
        "       [-0.04380995, -0.18179776]])\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "5wRXHKHbp4d5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3ece224c-0164-4a36-8f7f-078bb59f460f"
      },
      "cell_type": "code",
      "source": [
        "ar = np.zeros([3, 2])\n",
        "ar"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "403xa6LMeD4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_n = 3    #how many input nodes\n",
        "output_n = 2   #how many output nodes\n",
        "sample_n = 10  #how many simulated samples to try\n",
        "\n",
        "np.random.seed(42)\n",
        "weights = .2*np.random.rand(input_n,output_n) - .1 #initial value of weights matrix\n",
        "history_cache = np.zeros([3, 2])\n",
        "\n",
        "print(weights)\n",
        "print('='*30)\n",
        "\n",
        "for i in range(sample_n):\n",
        "  #generate simulated weight changes as if coming from backprop\n",
        "  simulated_changes = .2*np.random.rand(input_n,output_n) - .1\n",
        "  past1 = history_cache.copy()\n",
        "  weights = weights + (-simulated_changes- (.09*past5))\n",
        "  history_cache = simulated_changes\n",
        "\n",
        "\n",
        "\n",
        "weights  #values when done with for loop"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}